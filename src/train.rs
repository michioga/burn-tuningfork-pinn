//! # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
//!
//! `LearnerBuilder`ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ§‹æˆã—ã€
//! ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

// â­ï¸ 1. physicsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
use crate::{
    constants::physics::BEAM_THEORY_CHOICE,
    dataset::{FemDataset, PinnBatcher},
    model::TuningForkPINN,
    physics,
};
use burn::{
    config::Config,
    data::dataloader::DataLoaderBuilder,
    lr_scheduler::constant::ConstantLr,
    module::Module,
    optim::AdamConfig,
    prelude::*,
    record::{CompactRecorder, Recorder},
    tensor::backend::{AutodiffBackend, Backend},
    train::{
        LearnerBuilder, RegressionOutput, TrainOutput, TrainStep, ValidStep,
        metric::{LearningRateMetric, LossMetric},
    },
};

/// ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®šç¾©ã—ã¾ã™ã€‚
///
/// ã“ã®å®Ÿè£…ã¯PINNã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å¿ƒè‡“éƒ¨ã§ã™ã€‚
/// `physics::tuning_fork_loss` ã‚’å‘¼ã³å‡ºã—ã€ãƒ‡ãƒ¼ã‚¿æå¤±ï¼ˆFEMãƒ‡ãƒ¼ã‚¿ã¨ã®èª¤å·®ï¼‰ã¨
/// ç‰©ç†æå¤±ï¼ˆç‰©ç†æ³•å‰‡ã¨ã®èª¤å·®ï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æå¤±ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
/// ã“ã®æå¤±ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ãƒ¼ã‚¿ã¸ã®å¿ å®Ÿåº¦ã¨ç‰©ç†æ³•å‰‡ã¸ã®æº–æ‹ ã‚’ä¸¡ç«‹ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã—ã¾ã™ã€‚
impl<B: AutodiffBackend> TrainStep<(Tensor<B, 2>, Tensor<B, 2>), RegressionOutput<B>>
    for TuningForkPINN<B>
{
    /// ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè£…
    fn step(&self, item: (Tensor<B, 2>, Tensor<B, 2>)) -> TrainOutput<RegressionOutput<B>> {
        let (freqs, targets) = item; // `targets` ãŒFEMã®æ­£è§£å¯¸æ³•
        let predicted_dims = self.forward(freqs.clone());

        // FEMãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚Šé‡è¦–ã—ãŸã„å ´åˆã€alphaã‚’0.5ãªã©ã«è¨­å®š
        const ALPHA: f32 = 0.5;

        let loss = physics::tuning_fork_loss(
            predicted_dims.clone(),
            freqs,
            targets.clone(), // æ­£è§£å¯¸æ³•ã‚’æ¸¡ã™
            ALPHA,           // alphaã‚’æ¸¡ã™
            BEAM_THEORY_CHOICE, // æ¢ç†è«–ã®é¸æŠã‚’æ¸¡ã™
        );

        let output = RegressionOutput {
            loss: loss.clone(),
            output: predicted_dims,
            targets,
        };
        TrainOutput::new(self, loss.backward(), output)
    }
}

/// ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®šç¾©ã—ã¾ã™ã€‚
///
/// å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã¨åŒæ§˜ã«ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æå¤±ï¼ˆãƒ‡ãƒ¼ã‚¿æå¤± + ç‰©ç†æå¤±ï¼‰ã‚’è¨ˆç®—ã—ã¦ã€
/// æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
impl<B: Backend> ValidStep<(Tensor<B, 2>, Tensor<B, 2>), RegressionOutput<B>>
    for TuningForkPINN<B>
{
    ///
    fn step(&self, item: (Tensor<B, 2>, Tensor<B, 2>)) -> RegressionOutput<B> {
        let (freqs, targets) = item;
        let predicted_dims = self.forward(freqs.clone());

        const ALPHA: f32 = 0.5;

        // ã“ã¡ã‚‰ã‚‚ç‰©ç†æå¤±é–¢æ•°ã§è©•ä¾¡ã™ã‚‹
        let loss = physics::tuning_fork_loss(
            predicted_dims.clone(),
            freqs,
            targets.clone(),
            ALPHA,
            BEAM_THEORY_CHOICE,
        );

        RegressionOutput {
            loss,
            output: predicted_dims,
            targets,
        }
    }
}

/// å­¦ç¿’è¨­å®š
#[derive(Config)]
pub struct TrainingConfig {
    /// ä½¿ç”¨ã™ã‚‹ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®è¨­å®š
    pub optimizer: AdamConfig,
    /// å­¦ç¿’ç‡
    #[config(default = 1e-4)]
    pub learning_rate: f64,
    /// å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°
    #[config(default = 300)]
    pub num_epochs: usize,
    /// ãƒãƒƒãƒã‚µã‚¤ã‚º
    #[config(default = 32)]
    pub batch_size: usize,
}

// ... run()é–¢æ•°ã¯å¤‰æ›´ãªã— ...
pub fn run<B: AutodiffBackend>(device: B::Device) {
    let config = TrainingConfig::new(AdamConfig::new());
    let artifact_dir = "./artifacts";
    std::fs::create_dir_all(artifact_dir).ok();

    let dataset_all = FemDataset::new("data/fem_data_augmented.csv");
    let dataset_train = dataset_all.clone();
    let dataset_valid = dataset_all;

    let batcher_train = PinnBatcher::<B>::new(device.clone());
    let dataloader_train = DataLoaderBuilder::new(batcher_train)
        .batch_size(config.batch_size)
        .num_workers(4)
        .build(dataset_train);

    let batcher_valid = PinnBatcher::<B::InnerBackend>::new(device.clone());
    let dataloader_valid = DataLoaderBuilder::new(batcher_valid)
        .batch_size(config.batch_size)
        .num_workers(4)
        .build(dataset_valid);

    let scheduler = ConstantLr::new(config.learning_rate);

    let learner = LearnerBuilder::new(artifact_dir)
        .devices(vec![device.clone()])
        .num_epochs(config.num_epochs)
        .metric_train_numeric(LossMetric::new())
        .metric_train_numeric(LearningRateMetric::new())
        .metric_valid_numeric(LossMetric::new())
        .build(
            TuningForkPINN::<B>::new(&device),
            config.optimizer.init(),
            scheduler,
        );

    println!("ğŸš€ Starting training on {:?}...", device);
    let model_trained = learner.fit(dataloader_train, dataloader_valid);

    let model_record = model_trained.into_record();
    CompactRecorder::new()
        .record(model_record, format!("{}/model", artifact_dir).into())
        .expect("Failed to save trained model");

    println!("\nâœ… Model saved to '{}/model.mpk'", artifact_dir);
}
